"""
API Routes - FIXED VERSION
Fixes certificate 404 errors by properly mapping both Job ID and Trace ID
"""

from fastapi import APIRouter, HTTPException, BackgroundTasks, Depends
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, validator
from typing import Optional, List, Dict, Any
import uuid
import logging
import json
import asyncio
from datetime import datetime

from sqlalchemy import select, desc, or_
from sqlalchemy.ext.asyncio import AsyncSession

# App Imports
from app.models.database import get_db, Repository
from app.agents.graph import run_analysis, get_analysis_status
from app.core.opik_config import OpikManager, MAIN_PROJECT
from app.core.config import settings
from opik import track, opik_context

from fastapi.responses import StreamingResponse
import asyncio

# Add this import at the top
from app.utils.sse import live_log_queues

# Initialize Router & Logger
router = APIRouter()
logger = logging.getLogger(__name__)

# ============================================================================
# REQUEST/RESPONSE MODELS
# ============================================================================

class FeedbackRequest(BaseModel):
    job_id: str
    score: int
    comment: Optional[str] = None

class AnalyzeRequest(BaseModel):
    repo_url: str
    user_id: Optional[str] = "anonymous"
    github_token: Optional[str] = None
    
    @validator('repo_url')
    def validate_repo_url(cls, v):
        if not v or not v.strip():
            raise ValueError("Repository URL is required")
        v = v.strip()
        if not ('github.com' in v):
            raise ValueError("Only GitHub repositories are supported")
        return v

class AnalyzeResponse(BaseModel):
    job_id: str
    status: str
    message: str
    estimated_time_seconds: int = 60

class StatusResponse(BaseModel):
    job_id: str
    status: str
    current_step: Optional[str] = None
    progress: int
    errors: list[str] = []
    final_credits: Optional[float] = None
    validation: Optional[dict] = None

class ResultResponse(BaseModel):
    job_id: str
    verification_id: str
    repo_url: str
    final_credits: float
    sfia_level: Optional[int] = None
    sfia_level_name: Optional[str] = None
    opik_trace_url: Optional[str] = None
    validation: Optional[dict] = None
    scan_metrics: Optional[dict] = None
    audit_result: Optional[dict] = None
    errors: list[str] = []
    started_at: Optional[str] = None
    completed_at: Optional[str] = None
    sfia_result: Optional[dict] = None
    validation_result: Optional[dict] = None
    mentorship_plan: Optional[dict] = None

# ============================================================================
# IN-MEMORY STORAGE
# ============================================================================
analysis_jobs = {}

# ============================================================================
# ENDPOINT 1: START ANALYSIS
# ============================================================================

@router.post("/analyze", response_model=AnalyzeResponse)
@track(name="API: Start Analysis", project_name=MAIN_PROJECT)
async def analyze_repository(
    request: AnalyzeRequest,
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db)
):
    # 1. Get the Trace ID generated by Opik (UUID v7)
    current_trace = opik_context.get_current_trace_data()
    
    # 2. Use Opik's ID as our PRIMARY job_id to ensure feedback works
    if current_trace and current_trace.id:
        job_id = current_trace.id
    else:
        job_id = str(uuid.uuid4()) # Fallback only

    logger.info(f"üöÄ Analysis starting with ID: {job_id}")

    job_data = {
        "job_id": job_id,
        "opik_trace_id": job_id, 
        "repo_url": request.repo_url,
        "user_id": request.user_id,
        "status": "queued",
        "has_user_token": bool(request.github_token),
        "validation": None,
        "errors": []
    }
    
    # 3. Store in memory (Only one entry needed because job_id == trace_id)
    analysis_jobs[job_id] = job_data
    
    async def run_in_background():
        try:
            final_state = await run_analysis(
                repo_url=request.repo_url,
                user_id=request.user_id,
                job_id=job_id,
                user_github_token=request.github_token
            )
            
            # ‚úÖ FIX: Removed the undefined 'trace_id' reference.
            # If the graph updated the trace ID internally, map it here.
            new_trace_id = final_state.get("opik_trace_id")
            if new_trace_id and new_trace_id != job_id:
                analysis_jobs[new_trace_id] = job_data
            
            if final_state.get("should_skip"):
                job_data.update({
                    "status": "failed",
                    "validation": final_state.get("validation"),
                    "errors": final_state.get("errors", []),
                    "skip_reason": final_state.get("skip_reason")
                })
            else:
                job_data.update({
                    "status": "complete",
                    "result": final_state
                })
            
        except Exception as e:
            logger.error(f"‚ùå Analysis failed: {str(e)}")
            job_data.update({
                "status": "error",
                "error": str(e),
                "errors": [str(e)]
            })
    
    background_tasks.add_task(run_in_background)
    
    return AnalyzeResponse(
        job_id=job_id,
        status="queued",
        message="Analysis started. Poll /status/{job_id} for progress.",
        estimated_time_seconds=60
    )

# ============================================================================
# ENDPOINT 2: CHECK STATUS
# ============================================================================

@router.get("/status/{job_id}", response_model=StatusResponse)
async def get_status(job_id: str):
    if job_id not in analysis_jobs:
        raise HTTPException(status_code=404, detail="Job not found")
    
    job = analysis_jobs[job_id]
    
    if job["status"] == "failed":
        return StatusResponse(
            job_id=job_id, status="failed", progress=10,
            errors=job.get("errors", []), validation=job.get("validation")
        )
    
    if job["status"] == "error":
        return StatusResponse(
            job_id=job_id, status="error", progress=0,
            errors=[job.get("error", "Unknown error")]
        )
    
    if job["status"] == "complete":
        result = job.get("result", {})
        return StatusResponse(
            job_id=job_id, status="complete", current_step="complete",
            progress=100, errors=result.get("errors", []),
            final_credits=result.get("final_credits"), validation=result.get("validation")
        )
    
    status = await get_analysis_status(job_id)
    return StatusResponse(job_id=job_id, **status)

# ============================================================================
# ENDPOINT 3: GET FINAL RESULT (FIXED - DUAL LOOKUP)
# ============================================================================

@router.get("/result/{job_id}", response_model=ResultResponse)
async def get_result(job_id: str, db: AsyncSession = Depends(get_db)):
    """
    ‚úÖ FIXED: Checks both in-memory AND database with dual ID lookup
    """
    # 1. Check in-memory first
    if job_id in analysis_jobs:
        job = analysis_jobs[job_id]
        if job["status"] == "complete":
            return _format_state_to_response(job_id, job["repo_url"], job["result"])
    
    # 2. ‚úÖ FIX: Database lookup with OR condition for both IDs
    query = select(Repository).where(
        or_(
            Repository.id == job_id,
            Repository.opik_trace_id == job_id,
            Repository.repo_fingerprint == job_id  # Also check fingerprint
        )
    ).order_by(desc(Repository.created_at))  # Get most recent if duplicates
    
    result_db = await db.execute(query)
    record = result_db.scalars().first()

    if record:
        workspace = getattr(settings, 'OPIK_WORKSPACE', 'default')
        return ResultResponse(
            job_id=record.id,
            verification_id=record.id,
            repo_url=record.repo_url,
            final_credits=record.final_credits or 0.0,
            sfia_level=record.sfia_level,
            sfia_level_name=_get_level_name(record.sfia_level),
            opik_trace_url=f"https://www.comet.com/{workspace}/opik/traces/{record.opik_trace_id}",
            validation=record.validation_result,
            scan_metrics=record.scan_metrics,
            audit_result=record.audit_result,
            sfia_result=record.sfia_result,
            validation_result=record.validation_result,
            errors=record.errors or [],
            started_at=record.created_at.isoformat() if record.created_at else None,
            completed_at=record.analyzed_at.isoformat() if record.analyzed_at else None
        )

    raise HTTPException(status_code=404, detail=f"Analysis result not found for ID: {job_id}")

# ============================================================================
# ENDPOINT 4: USER HISTORY
# ============================================================================

@router.get("/user/{user_id}/history")
async def get_user_history(user_id: str, db: AsyncSession = Depends(get_db)):
    """
    ‚úÖ FIXED: Fetches from DATABASE (persistent) instead of Opik
    """
    try:
        query = select(Repository).where(
            Repository.user_id == user_id
        ).order_by(desc(Repository.created_at))
        
        result = await db.execute(query)
        records = result.scalars().all()
        
        history = []
        for record in records:
            history.append({
                "id": record.id,
                "job_id": record.id,
                "repo_url": record.repo_url,
                "final_credits": float(record.final_credits or 0),
                "sfia_level": record.sfia_level or 0,
                "sfia_level_name": _get_level_name(record.sfia_level),
                "created_at": record.created_at.isoformat() if record.created_at else None,
                "opik_trace_url": f"https://www.comet.com/{settings.OPIK_WORKSPACE}/opik/traces/{record.opik_trace_id}" if record.opik_trace_id else None
            })
        
        return history
        
    except Exception as e:
        logger.error(f"History fetch failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# ENDPOINT 5: FEEDBACK
# ============================================================================

@router.post("/feedback")
async def log_feedback(request: FeedbackRequest):
    """Submit user feedback"""
    # ‚úÖ Add debug logging
    logger.info(f"Received feedback: job_id={request.job_id}, score={request.score}, comment={request.comment}")
    
    try:
        client = OpikManager.get_client(MAIN_PROJECT)
        client.log_traces_feedback_scores(scores=[{
            "id": request.job_id, 
            "name": "user_satisfaction",
            "value": float(request.score), 
            "reason": request.comment or "Feedback"
        }])
        
        logger.info(f"Feedback logged successfully to Opik")
        return {"status": "recorded", "message": "Thank you for your feedback!"}
        
    except Exception as e:
        logger.error(f"Feedback submission failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# HELPERS
# ============================================================================

def _get_level_name(level):
    mapping = {1: "Follow", 2: "Assist", 3: "Apply", 4: "Enable", 5: "Ensure"}
    return mapping.get(int(level or 0), "Unknown")

def _format_state_to_response(job_id, repo_url, result):
    sfia = result.get("sfia_result") or {}
    workspace = getattr(settings, 'OPIK_WORKSPACE', 'default')
    v7_id = result.get("opik_trace_id", job_id)
    return ResultResponse(
        job_id=job_id, verification_id=v7_id, repo_url=repo_url, final_credits=result.get("final_credits", 0.0),
        sfia_level=sfia.get("sfia_level"), sfia_level_name=sfia.get("level_name"),
        opik_trace_url=f"https://www.comet.com/{workspace}/opik/traces/{result.get('opik_trace_id', job_id)}",
        validation=result.get("validation"), scan_metrics=result.get("scan_metrics"),
        audit_result=result.get("audit_result"),
        mentorship_plan=result.get("mentorship_plan"), sfia_result=sfia,
        validation_result=result.get("validation_result"), errors=result.get("errors", []),
        started_at=result.get("started_at"), completed_at=result.get("completed_at")
    )

@router.get("/jobs")
async def list_jobs():
    return {"total_jobs": len(analysis_jobs), "jobs": [{"id": k, "status": v["status"]} for k,v in analysis_jobs.items()]}

# Add this to backend/app/api/routes.py



# Add this endpoint BEFORE the existing endpoints

@router.get("/stream/{job_id}")
async def stream_live_logs(job_id: str):
    """SSE endpoint for streaming live agent logs to frontend"""
    async def event_generator():
        if job_id not in live_log_queues:
            live_log_queues[job_id] = asyncio.Queue()
        
        queue = live_log_queues[job_id]
        
        try:
            while True:
                try:
                    log = await asyncio.wait_for(queue.get(), timeout=30.0)
                    yield f"data: {json.dumps(log)}\n\n"
                    
                    # ‚úÖ ADD: Check if job is complete
                    if log.get("agent") == "reporter" and log.get("status") == "success":
                        # Send final message and close
                        yield f"data: {json.dumps({'event': 'complete'})}\n\n"
                        break
                        
                except asyncio.TimeoutError:
                    # Send keep-alive ping
                    yield f": keepalive\n\n"
                    
                    # ‚úÖ ADD: Check if job is done via status
                    status = await get_analysis_status(job_id)
                    if status.get("status") == "complete":
                        yield f"data: {json.dumps({'event': 'complete'})}\n\n"
                        break
                        
        except asyncio.CancelledError:
            pass
        finally:
            if job_id in live_log_queues:
                del live_log_queues[job_id]
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )